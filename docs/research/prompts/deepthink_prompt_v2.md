# Deep Think v2 Evaluation Prompt

> **Usage**: Upload `docs/deepthink_context_v2.md` as an attachment, then paste the prompt below.

---

## The Prompt

```
The attached file contains the complete source code of "context-builder" v0.7.0, a CLI tool that packages codebases into a single markdown file optimized for LLM consumption. This file was generated by the tool itself.

IMPORTANT CONTEXT: You previously reviewed v0.6.0 of this tool and identified 5 bugs:
1. Cache TOCTOU data loss (File::create truncating before lock) — FIXED
2. Indentation destruction in diff_only mode (trim_start) — FIXED
3. UTF-8 8KB boundary corruption — FIXED
4. CLI flags silently overwritten by config — FIXED
5. Double file seek (copy-paste error) — FIXED

Since your last review, three new features have been added (v0.7.0):
- **Deterministic output**: Timestamps replaced with content hashes for LLM prompt caching
- **Relevance-based file ordering**: Files sorted by category (config → source → tests → docs) instead of alphabetically
- **`--max-tokens N` context budgeting**: Caps output size to prevent API errors

The file you're reading RIGHT NOW uses relevance ordering. Notice: config files (Cargo.toml) appear first, then source code (src/*), then tests (tests/*), then documentation (*.md) at the end.

---

Please perform the following analysis. Take your time — depth matters more than speed.

## Part 1: Relevance Ordering Impact Assessment

You are reading relevance-ordered output for the first time. Compare this experience to reading alphabetically-ordered code:

1. **Comprehension Impact**: Did seeing config and core source files first improve your ability to build a mental model of the project? Did you find yourself needing to "look ahead" less?
2. **Category Boundaries**: The ordering is config(0) → source(1) → tests(2) → docs(3). Is this optimal? Would you prefer a different ordering? (e.g., docs first for context? tests closer to source?)
3. **Within-Category Alphabetical**: Within each relevance category, files are alphabetically sorted. Would a different intra-category ordering be better? (e.g., by dependency graph, by file size, by change frequency?)
4. **Missing Categories**: Are there file types that should be in their own category? (e.g., build scripts, CI configs, migration files?)

## Part 2: Architecture & Code Review (Delta from v0.6.0)

Focus on the NEW code added in v0.7.0:

1. **Content Hash Implementation** (markdown.rs): Review the `DefaultHasher`-based hashing. Is it collision-resistant enough? Are the right inputs being hashed? Any ordering concerns?
2. **Token Budgeting** (markdown.rs, cli.rs, config_resolver.rs): Review the `max_tokens` implementation. Is the 4-bytes-per-token estimation reasonable? Are there edge cases in the truncation logic?
3. **Relevance Sorting** (file_utils.rs): Review `file_relevance_category()`. Are the heuristics correct? Any misclassifications you'd expect? Is the category assignment complete?
4. **Remaining Bugs**: Are there any NEW bugs introduced by these features? Check for: race conditions, edge cases in category assignment, hash collisions, truncation boundary issues.
5. **Bug Fix Verification**: Can you confirm the 5 fixes from v0.6.1 look correct based on the current code?

## Part 3: Strategic Feature Roadmap (Updated)

Given that Tier 1 is now shipped, propose the 5 most impactful features for Tier 2, ordered by value. For each:

- **Problem it solves** (with concrete user scenario)
- **Technical design** (which files change, new modules needed)
- **Complexity estimate** (S/M/L) and why
- **Risk factors** (what could go wrong)

Focus on features that would make YOU (the LLM consumer) significantly better at understanding codebases. The goal is to maximize the quality of reasoning per token consumed.

## Part 4: Output Format v2 Specification

Design the ideal next-generation output format. Be opinionated and concrete:

1. **Header metadata**: What project-level metadata should precede the code?
2. **Per-file metadata**: What should appear before each file's content? (dependency list? purpose annotation? complexity score?)
3. **Inter-file relationships**: How should imports, call graphs, or data flow between files be represented?
4. **Summary layers**: Should there be an architectural summary or module overview before raw code?
5. **Token efficiency**: What could be removed or compressed without losing signal?

Provide a concrete example of what the first ~200 lines of an ideal v2 output would look like for this project.
```

---

## What's Different from v1

| Aspect | v1 Prompt | v2 Prompt |
|--------|-----------|-----------|
| **Context** | Cold review, no prior knowledge | Warm review, knows about 5 fixed bugs |
| **Focus** | General analysis + bug hunting | Delta analysis + new feature validation |
| **Part 1** | Generic format evaluation | Specific relevance ordering A/B comparison |
| **Part 3** | Open-ended roadmap | Tier 2 roadmap (Tier 1 shipped) |
| **File ordering** | Alphabetical | Relevance-based (config → source → tests → docs) |
